{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from typing import *\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"data/sarcasm/sarcasm.json\")\n",
    "df = pd.read_json(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26709 entries, 0 to 26708\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   article_link  26709 non-null  object\n",
      " 1   headline      26709 non-null  object\n",
      " 2   is_sarcastic  26709 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 626.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.huffingtonpost.com/entry/versace-black-code_us_5861fbefe4b0de3a08f600d5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article_link'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>webapp_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>theonion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>huffingtonpost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \\\n",
       "0  former versace store clerk sues over secret 'b...             0   \n",
       "1  the 'roseanne' revival catches up to our thorn...             0   \n",
       "2  mom starting to fear son's web series closest ...             1   \n",
       "3  boehner just wants wife to listen, not come up...             1   \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
       "\n",
       "      webapp_name  \n",
       "0  huffingtonpost  \n",
       "1  huffingtonpost  \n",
       "2        theonion  \n",
       "3        theonion  \n",
       "4  huffingtonpost  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def webapp_name(x):\n",
    "    return re.findall(r'\\.(.*?)\\.', x)[0]\n",
    "\n",
    "\n",
    "df['webapp_name'] = df['article_link'].apply(webapp_name)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "webapp_name\n",
       "huffingtonpost    0.561047\n",
       "theonion          0.438953\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['webapp_name'].value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each line to a list\n",
    "sentences = df['headline'].to_list()\n",
    "labels = df['is_sarcastic'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"former versace store clerk sues over secret 'black code' for minority shoppers\",\n",
       "  \"the 'roseanne' revival catches up to our thorny political mood, for better and worse\",\n",
       "  \"mom starting to fear son's web series closest thing she will have to grandchild\"],\n",
       " [0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:3], labels[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(sentences: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Perform text cleaning on a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): List of input sentences.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of cleaned sentences after applying stemming, lowercasing,\n",
    "                   and removing stopwords.\n",
    "\n",
    "    Note:\n",
    "        This function uses the NLTK library for stemming and stopwords removal.\n",
    "        Make sure to have NLTK installed (`pip install nltk`) and download the\n",
    "        stopwords corpus using `nltk.download('stopwords')`.\n",
    "    \"\"\"\n",
    "    ps = PorterStemmer()\n",
    "    corpus = []\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        each_sentence = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "        each_sentence = each_sentence.lower()\n",
    "        each_sentence = each_sentence.split()\n",
    "\n",
    "        each_sentence = [ps.stem(word)\n",
    "                         for word in each_sentence if word not in stopwords.words('english')]\n",
    "        each_sentence = ' '.join(each_sentence)\n",
    "        corpus.append(each_sentence)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each word contains only a to z in alphabets\n",
    "* All stopwords are removed\n",
    "* All words are converted to root word with help of PorterSTemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentences = clean_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['former versac store clerk sue secret black code minor shopper',\n",
       " 'roseann reviv catch thorni polit mood better wors',\n",
       " 'mom start fear son web seri closest thing grandchild']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 0\n",
    "for each_sentence in clean_sentences:\n",
    "    temp_len = len(each_sentence)\n",
    "    if MAX_LENGTH<temp_len:\n",
    "        MAX_LENGTH = temp_len\n",
    "\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Train test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    clean_sentences, labels, test_size=0.20, random_state=0, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share hazmat suit maker spike nyc ebola news',\n",
       " 'miss america call u n council promot enough world peac',\n",
       " 'tarsier world smallest primat anim planet looney front part']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.561052\n",
       "1    0.438948\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_train).value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.561026\n",
       "1    0.438974\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).value_counts(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tokenization, Sequences and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\91981\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary size of the tokenizer\n",
    "VOCAB_SIZE = 10000\n",
    "\n",
    "# Maximum length of clean sentence is  217\n",
    "# Giving 50 as the max length including the padded sequences\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "# Output dimensions of the Embedding layer\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "# Parameters for padding and OOV tokens\n",
    "TRUNC_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'\n",
    "OOV_TOKEN = \"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['share hazmat suit maker spike nyc ebola news',\n",
       " 'miss america call u n council promot enough world peac',\n",
       " 'tarsier world smallest primat anim planet looney front part']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(VOCAB_SIZE, OOV_TOKEN, X_data=X_train):\n",
    "    # Initialize the Tokenizer class\n",
    "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=OOV_TOKEN)\n",
    "\n",
    "    # Generate the word index dictionary\n",
    "    tokenizer.fit_on_texts(X_data)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def to_sequences_n_padding(tokenizer,MAX_LENGTH,PADDING_TYPE,TRUNC_TYPE,X_data):\n",
    "    # Generate and pad the training sequences\n",
    "    _sequences = tokenizer.texts_to_sequences(X_data)\n",
    "    _padded = pad_sequences(_sequences, \n",
    "                            maxlen=MAX_LENGTH, \n",
    "                            padding=PADDING_TYPE, \n",
    "                            truncating=TRUNC_TYPE)\n",
    "    \n",
    "    return _padded   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(VOCAB_SIZE=VOCAB_SIZE,OOV_TOKEN=OOV_TOKEN)\n",
    "\n",
    "training_padded = to_sequences_n_padding(tokenizer=tokenizer,\n",
    "                                            MAX_LENGTH=MAX_LENGTH,\n",
    "                                            PADDING_TYPE=PADDING_TYPE,\n",
    "                                            TRUNC_TYPE=TRUNC_TYPE,\n",
    "                                            X_data=X_train)\n",
    "testing_padded = to_sequences_n_padding(tokenizer=tokenizer,\n",
    "                                            MAX_LENGTH=MAX_LENGTH,\n",
    "                                            PADDING_TYPE=PADDING_TYPE,\n",
    "                                            TRUNC_TYPE=TRUNC_TYPE,\n",
    "                                            X_data=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained tokenizer saved to tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "preprocessor_file_path = 'preprocessor/tokenizer.pkl'\n",
    "\n",
    "# Extract the directory and filename\n",
    "file_dir, file_name = os.path.split(preprocessor_file_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(file_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained tokenizer to a pickle file\n",
    "with open(preprocessor_file_path, 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "print(\"Trained tokenizer saved to tokenizer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels lists into numpy arrays\n",
    "training_labels = np.array(y_train)\n",
    "testing_labels = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21367, 50)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5342, 50)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (training_padded, training_labels))\n",
    "train_dataset = (train_dataset\n",
    "                 .shuffle(buffer_size=8, reshuffle_each_iteration=True)\n",
    "                 .batch(32)\n",
    "                 .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (testing_padded, testing_labels))\n",
    "test_dataset = (test_dataset\n",
    "                .shuffle(buffer_size=8, reshuffle_each_iteration=True)\n",
    "                .batch(32)\n",
    "                .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. RNN +BIDIRECTIONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.0001,\n",
    "    patience=5,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    # baseline=None,\n",
    "    restore_best_weights=True,\n",
    "    # start_from_epoch=0\n",
    ")\n",
    "\n",
    "# Saving the best model and its weights to given path\n",
    "checkpoint_filepath = 'models/model_checkpoint_RNN.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    verbose=1,\n",
    "    # save_weights_only=True,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 16)            160000    \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirecti  (None, 50, 60)            2820      \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 100)               11100     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense32 (Dense)             (None, 120)               12120     \n",
      "                                                                 \n",
      " batch1 (BatchNormalization  (None, 120)               480       \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout1 (Dropout)          (None, 120)               0         \n",
      "                                                                 \n",
      " dense16 (Dense)             (None, 80)                9680      \n",
      "                                                                 \n",
      " batch2 (BatchNormalization  (None, 80)                320       \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout2 (Dropout)          (None, 80)                0         \n",
      "                                                                 \n",
      " last_dense1 (Dense)         (None, 1)                 81        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196601 (767.97 KB)\n",
      "Trainable params: 196201 (766.41 KB)\n",
      "Non-trainable params: 400 (1.56 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(\n",
    "        VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH, name=\"embedding\"),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(30, name=\"rnn1_30\", return_sequences=True)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(50, name=\"rnn2_50\", return_sequences=False)),\n",
    "\n",
    "    # tf.keras.layers.Flatten(),\n",
    "\n",
    "    tf.keras.layers.Dense(120, activation='relu', name=\"dense32\"),\n",
    "    tf.keras.layers.BatchNormalization(name=\"batch1\"),\n",
    "    tf.keras.layers.Dropout(0.5, name=\"dropout1\"),\n",
    "\n",
    "    tf.keras.layers.Dense(80, activation='relu', name=\"dense16\"),\n",
    "    tf.keras.layers.BatchNormalization(name=\"batch2\"),\n",
    "    tf.keras.layers.Dropout(0.4, name=\"dropout2\"),\n",
    "\n",
    "\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', name=\"last_dense1\")\n",
    "])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.74031, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 29s - loss: 0.9624 - binary_accuracy: 0.5036 - val_loss: 0.7403 - val_binary_accuracy: 0.5225 - 29s/epoch - 43ms/step\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 0.74031 to 0.72541, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.9183 - binary_accuracy: 0.5166 - val_loss: 0.7254 - val_binary_accuracy: 0.5399 - 21s/epoch - 31ms/step\n",
      "Epoch 3/30\n",
      "\n",
      "Epoch 3: val_loss improved from 0.72541 to 0.71145, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 22s - loss: 0.8913 - binary_accuracy: 0.5318 - val_loss: 0.7115 - val_binary_accuracy: 0.5547 - 22s/epoch - 34ms/step\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss improved from 0.71145 to 0.69988, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 22s - loss: 0.8676 - binary_accuracy: 0.5381 - val_loss: 0.6999 - val_binary_accuracy: 0.5681 - 22s/epoch - 32ms/step\n",
      "Epoch 5/30\n",
      "\n",
      "Epoch 5: val_loss did not improve from 0.69988\n",
      "668/668 - 21s - loss: 0.8537 - binary_accuracy: 0.5398 - val_loss: 0.7137 - val_binary_accuracy: 0.5623 - 21s/epoch - 31ms/step\n",
      "Epoch 6/30\n",
      "\n",
      "Epoch 6: val_loss improved from 0.69988 to 0.69226, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.8408 - binary_accuracy: 0.5470 - val_loss: 0.6923 - val_binary_accuracy: 0.5861 - 21s/epoch - 32ms/step\n",
      "Epoch 7/30\n",
      "\n",
      "Epoch 7: val_loss improved from 0.69226 to 0.68719, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 22s - loss: 0.8231 - binary_accuracy: 0.5559 - val_loss: 0.6872 - val_binary_accuracy: 0.5923 - 22s/epoch - 33ms/step\n",
      "Epoch 8/30\n",
      "\n",
      "Epoch 8: val_loss did not improve from 0.68719\n",
      "668/668 - 21s - loss: 0.8231 - binary_accuracy: 0.5546 - val_loss: 0.7032 - val_binary_accuracy: 0.5726 - 21s/epoch - 32ms/step\n",
      "Epoch 9/30\n",
      "\n",
      "Epoch 9: val_loss improved from 0.68719 to 0.68408, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.8084 - binary_accuracy: 0.5589 - val_loss: 0.6841 - val_binary_accuracy: 0.5914 - 21s/epoch - 32ms/step\n",
      "Epoch 10/30\n",
      "\n",
      "Epoch 10: val_loss improved from 0.68408 to 0.66579, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.7956 - binary_accuracy: 0.5719 - val_loss: 0.6658 - val_binary_accuracy: 0.6101 - 21s/epoch - 31ms/step\n",
      "Epoch 11/30\n",
      "\n",
      "Epoch 11: val_loss improved from 0.66579 to 0.66097, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 24s - loss: 0.7818 - binary_accuracy: 0.5751 - val_loss: 0.6610 - val_binary_accuracy: 0.6121 - 24s/epoch - 36ms/step\n",
      "Epoch 12/30\n",
      "\n",
      "Epoch 12: val_loss did not improve from 0.66097\n",
      "668/668 - 23s - loss: 0.7715 - binary_accuracy: 0.5865 - val_loss: 0.6620 - val_binary_accuracy: 0.6191 - 23s/epoch - 34ms/step\n",
      "Epoch 13/30\n",
      "\n",
      "Epoch 13: val_loss improved from 0.66097 to 0.65792, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 22s - loss: 0.7528 - binary_accuracy: 0.5939 - val_loss: 0.6579 - val_binary_accuracy: 0.6292 - 22s/epoch - 33ms/step\n",
      "Epoch 14/30\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.65792\n",
      "668/668 - 21s - loss: 0.7461 - binary_accuracy: 0.5978 - val_loss: 0.6594 - val_binary_accuracy: 0.6327 - 21s/epoch - 32ms/step\n",
      "Epoch 15/30\n",
      "\n",
      "Epoch 15: val_loss improved from 0.65792 to 0.64102, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 27s - loss: 0.7232 - binary_accuracy: 0.6120 - val_loss: 0.6410 - val_binary_accuracy: 0.6453 - 27s/epoch - 40ms/step\n",
      "Epoch 16/30\n",
      "\n",
      "Epoch 16: val_loss improved from 0.64102 to 0.63825, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 22s - loss: 0.7151 - binary_accuracy: 0.6207 - val_loss: 0.6383 - val_binary_accuracy: 0.6524 - 22s/epoch - 33ms/step\n",
      "Epoch 17/30\n",
      "\n",
      "Epoch 17: val_loss improved from 0.63825 to 0.63011, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 20s - loss: 0.7059 - binary_accuracy: 0.6337 - val_loss: 0.6301 - val_binary_accuracy: 0.6636 - 20s/epoch - 31ms/step\n",
      "Epoch 18/30\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.63011\n",
      "668/668 - 20s - loss: 0.6925 - binary_accuracy: 0.6398 - val_loss: 0.6453 - val_binary_accuracy: 0.6522 - 20s/epoch - 30ms/step\n",
      "Epoch 19/30\n",
      "\n",
      "Epoch 19: val_loss improved from 0.63011 to 0.62192, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.6730 - binary_accuracy: 0.6506 - val_loss: 0.6219 - val_binary_accuracy: 0.6750 - 21s/epoch - 31ms/step\n",
      "Epoch 20/30\n",
      "\n",
      "Epoch 20: val_loss did not improve from 0.62192\n",
      "668/668 - 20s - loss: 0.6670 - binary_accuracy: 0.6557 - val_loss: 0.6407 - val_binary_accuracy: 0.6614 - 20s/epoch - 30ms/step\n",
      "Epoch 21/30\n",
      "\n",
      "Epoch 21: val_loss improved from 0.62192 to 0.61466, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 20s - loss: 0.6579 - binary_accuracy: 0.6657 - val_loss: 0.6147 - val_binary_accuracy: 0.6853 - 20s/epoch - 30ms/step\n",
      "Epoch 22/30\n",
      "\n",
      "Epoch 22: val_loss did not improve from 0.61466\n",
      "668/668 - 20s - loss: 0.6380 - binary_accuracy: 0.6768 - val_loss: 0.6402 - val_binary_accuracy: 0.6683 - 20s/epoch - 30ms/step\n",
      "Epoch 23/30\n",
      "\n",
      "Epoch 23: val_loss did not improve from 0.61466\n",
      "668/668 - 20s - loss: 0.6278 - binary_accuracy: 0.6865 - val_loss: 0.6163 - val_binary_accuracy: 0.6851 - 20s/epoch - 30ms/step\n",
      "Epoch 24/30\n",
      "\n",
      "Epoch 24: val_loss improved from 0.61466 to 0.60297, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 20s - loss: 0.6150 - binary_accuracy: 0.6969 - val_loss: 0.6030 - val_binary_accuracy: 0.6945 - 20s/epoch - 29ms/step\n",
      "Epoch 25/30\n",
      "\n",
      "Epoch 25: val_loss improved from 0.60297 to 0.59051, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 20s - loss: 0.6004 - binary_accuracy: 0.7008 - val_loss: 0.5905 - val_binary_accuracy: 0.7037 - 20s/epoch - 30ms/step\n",
      "Epoch 26/30\n",
      "\n",
      "Epoch 26: val_loss did not improve from 0.59051\n",
      "668/668 - 22s - loss: 0.5890 - binary_accuracy: 0.7123 - val_loss: 0.6137 - val_binary_accuracy: 0.6919 - 22s/epoch - 33ms/step\n",
      "Epoch 27/30\n",
      "\n",
      "Epoch 27: val_loss did not improve from 0.59051\n",
      "668/668 - 21s - loss: 0.5774 - binary_accuracy: 0.7192 - val_loss: 0.5920 - val_binary_accuracy: 0.7055 - 21s/epoch - 32ms/step\n",
      "Epoch 28/30\n",
      "\n",
      "Epoch 28: val_loss improved from 0.59051 to 0.58602, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 21s - loss: 0.5695 - binary_accuracy: 0.7257 - val_loss: 0.5860 - val_binary_accuracy: 0.7108 - 21s/epoch - 32ms/step\n",
      "Epoch 29/30\n",
      "\n",
      "Epoch 29: val_loss improved from 0.58602 to 0.56918, saving model to models\\model_checkpoint_RNN.h5\n",
      "668/668 - 20s - loss: 0.5512 - binary_accuracy: 0.7379 - val_loss: 0.5692 - val_binary_accuracy: 0.7235 - 20s/epoch - 30ms/step\n",
      "Epoch 30/30\n",
      "\n",
      "Epoch 30: val_loss did not improve from 0.56918\n",
      "668/668 - 20s - loss: 0.5463 - binary_accuracy: 0.7405 - val_loss: 0.6002 - val_binary_accuracy: 0.7050 - 20s/epoch - 29ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d428649690>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(train_dataset,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=30,\n",
    "          verbose=2,\n",
    "          callbacks=[early_stopping, \n",
    "                     model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(history.history['loss'], label=\"train_loss\")\n",
    "ax[0].plot(history.history['val_loss'], label=\"val_loss\")\n",
    "ax[0].set_title(\"SCE loss function\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(history.history['binary_accuracy'], label=\"train_acc\")\n",
    "ax[1].plot(history.history['val_binary_accuracy'], label=\"val_acc\")\n",
    "ax[1].set_title(\"Accuracy Metric function\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
